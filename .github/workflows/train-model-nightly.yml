name: Nightly Train Win/Loss Model from S3

on:
  schedule:
    # 05:30 UTC = 00:30 AM ET, tweak as needed
    - cron: "30 5 * * *"
  workflow_dispatch: {}

jobs:
  train-model:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # if you have a requirements.txt at repo root for sheets/etc:
          if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
          fi
          # install engine-specific deps
          pip install -r alpha_signal_engine/requirements.txt
          # make sure mlflow + boto3 + telegram are present (like in Dockerfile.trainer)
          pip install mlflow boto3 "python-telegram-bot==13.15" python-dotenv


      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-2
          
          
      - name: Debug AWS identity and S3 access
        run: |
          echo "Caller identity:"
          aws sts get-caller-identity
          echo "Listing a few observation files:"
          aws s3 ls s3://sharpsignal-ml-data/raw/all_observations/ --recursive | head


      - name: Download latest all_observations CSV from S3
        env:
          ML_DATA_BUCKET: sharpsignal-ml-data
        run: |
          mkdir -p alpha_signal_engine/data

          LATEST_KEY=$(aws s3 ls s3://$ML_DATA_BUCKET/raw/all_observations/ --recursive \
            | sort \
            | awk '/all_observations_.*\.csv$/ {print $4}' \
            | tail -n 1)

          echo "Latest observations key: $LATEST_KEY"

          if [ -z "$LATEST_KEY" ]; then
            echo "No all_observations_*.csv found in S3; failing."
            exit 1
          fi

          aws s3 cp "s3://$ML_DATA_BUCKET/$LATEST_KEY" \
            "alpha_signal_engine/data/ConfirmedBets - AllObservations.csv"
            
            
      - name: Reset MLflow local store
        working-directory: alpha_signal_engine
        run: |
          rm -rf mlruns

      - name: Run train_model.py (chronological split + MLflow + S3 upload)
        working-directory: alpha_signal_engine
        env:
          MLFLOW_TRACKING_URI: "file:./mlruns"
          MLFLOW_EXPERIMENT_NAME: "sharpsignal-winloss-ci"   # <- new name
          S3_MODEL_BUCKET: "sharpsignal-ml-data"
          S3_MODELS_PREFIX: "models/prod"
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          DRIFT_ALERT_CHAT_ID: ${{ secrets.DRIFT_ALERT_CHAT_ID }}
        run: |
          python src/train_model.py \
            --dataset "ConfirmedBets - AllObservations.csv" \
            --chronological
